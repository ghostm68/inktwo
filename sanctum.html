<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>INKREALM // SANCTUM</title>
    <style>
        /* THEME: Black, Red, White, Grey */
        body {
            background-color: #000000;
            color: #ffffff;
            font-family: 'Courier New', Courier, monospace; /* Coding/Typewriter vibe */
            margin: 0;
            display: flex;
            flex-direction: column;
            height: 100vh;
            overflow: hidden;
        }

        /* HEADER */
        header {
            border-bottom: 2px solid #ff0000; /* Red Line */
            padding: 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        h1 { margin: 0; letter-spacing: 4px; font-size: 1.2rem; text-transform: uppercase; }
        .status { font-size: 0.8rem; color: #888; }
        .status.ready { color: #00ff00; } /* Green when ready */
        .status.error { color: #ff0000; }

        /* CHAT AREA */
        #chat-container {
            flex: 1;
            padding: 20px;
            overflow-y: auto;
            background-color: #111; /* Very dark grey */
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .message {
            max-width: 80%;
            padding: 15px;
            border-left: 3px solid #333;
            line-height: 1.5;
        }

        .user-msg {
            align-self: flex-end;
            background-color: #222;
            border-left: 3px solid #ffffff; /* White accent for user */
        }

        .ai-msg {
            align-self: flex-start;
            background-color: #000;
            border-left: 3px solid #ff0000; /* Red accent for AI */
            color: #ccc;
        }

        /* INPUT AREA */
        #input-area {
            padding: 20px;
            background-color: #000;
            border-top: 1px solid #333;
            display: flex;
            gap: 10px;
        }

        textarea {
            flex: 1;
            background-color: #111;
            border: 1px solid #333;
            color: #fff;
            padding: 10px;
            font-family: inherit;
            resize: none;
            outline: none;
            height: 50px;
        }

        textarea:focus { border-color: #ff0000; }

        button {
            background-color: #ff0000;
            color: #000;
            border: none;
            padding: 0 20px;
            font-weight: bold;
            cursor: pointer;
            text-transform: uppercase;
            transition: background 0.2s;
        }

        button:hover { background-color: #fff; }
        button:disabled { background-color: #333; color: #555; cursor: not-allowed; }

        /* Loading Bar */
        #progress-bar {
            height: 2px;
            background-color: #ff0000;
            width: 0%;
            transition: width 0.3s;
        }
    </style>
</head>
<body>

    <header>
        <h1>Inkrealm // Sanctum</h1>
        <div id="status" class="status">INITIALIZING NEURAL LINK...</div>
    </header>
    <div id="progress-bar"></div>

    <div id="chat-container">
        <div class="message ai-msg">
            System Online. The Scribe is listening.
        </div>
    </div>

    <div id="input-area">
        <textarea id="user-input" placeholder="Enter command..." disabled></textarea>
        <button id="send-btn" disabled>TRANSMIT</button>
    </div>

    <!-- GOOGLE MEDIAPIPE LOGIC -->
    <script type="module">
        import { LlmInference, FilesetResolver } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai';

        const output = document.getElementById('chat-container');
        const input = document.getElementById('user-input');
        const sendBtn = document.getElementById('send-btn');
        const status = document.getElementById('status');
        const progressBar = document.getElementById('progress-bar');

        let llm = null;

        // *** CONFIGURATION ***
        // MAKE SURE THIS MATCHES YOUR R2 SETUP EXACTLY
        const modelUrl = 'https://gemma.inkrealm.info/gemma3-270m-it-q8-web.task'; 

        async function initAI() {
            try {
                // 1. Load the Engine
                const genai = await FilesetResolver.forGenAiTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/wasm"
                );

                // 2. Load the Model (with progress bar)
                status.innerText = "DOWNLOADING MODEL (276MB)...";
                progressBar.style.width = "50%";

                llm = await LlmInference.createFromOptions(genai, {
                    baseOptions: { modelAssetPath: modelUrl },
                    maxTokens: 1000,
                    temperature: 0.7,
                    topK: 40
                });

                // 3. Ready
                progressBar.style.width = "100%";
                setTimeout(() => { progressBar.style.display = 'none'; }, 500);
                
                status.innerText = "CONNECTED";
                status.classList.add('ready');
                input.disabled = false;
                sendBtn.disabled = false;
                input.focus();

            } catch (err) {
                console.error(err);
                status.innerText = "CONNECTION FAILED: " + err.message;
                status.classList.add('error');
                output.innerHTML += `<div class="message ai-msg" style="color:red">ERROR: Could not load model from ${modelUrl}. <br>Check your R2 CORS settings or Zero Trust login.</div>`;
            }
        }

        // Handle User Input
        async function handleChat() {
            const text = input.value.trim();
            if (!text || !llm) return;

            // Add User Message
            appendMessage(text, 'user-msg');
            input.value = '';
            input.disabled = true; // Lock input while thinking

            try {
                // Add AI Placeholder
                const aiDiv = appendMessage("Thinking...", 'ai-msg');
                
                // Stream the response
                let fullResponse = "";
                await llm.generateResponse(text, (partialResult) => {
                    fullResponse += partialResult;
                    aiDiv.innerText = fullResponse;
                    // Auto-scroll to bottom
                    output.scrollTop = output.scrollHeight;
                });
                
            } catch (err) {
                appendMessage("Error generating response.", 'ai-msg');
            }

            input.disabled = false;
            input.focus();
        }

        function appendMessage(text, className) {
            const div = document.createElement('div');
            div.className = `message ${className}`;
            div.innerText = text;
            output.appendChild(div);
            output.scrollTop = output.scrollHeight;
            return div;
        }

        // Event Listeners
        sendBtn.addEventListener('click', handleChat);
        input.addEventListener('keypress', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                handleChat();
            }
        });

        // Start
        initAI();
    </script>
</body>
</html>
